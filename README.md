# Droidish
A Droidspeak-like encoder and decoder 

The encoding of English sentences into Droidish uses two overlapping systems. One that uses a dictionary to translate a defined set of common words into specific tones and a second that translates characters like the alphabet, numbers, and punctuation into specific tones. The combination of the two allows for a controllable level of compression by how many common words, or even combinations of words, are mapped to tones. Since the word dictionary needs a large number of mappable values, I used a system of encoding where each word is given a set of two tones, one after the other. This increased the number of mappable values from about 50, using single tones, to nearly 3,000 with tone pairs. I've found that a word dictionary with the most common 1,000 words provides an adequate amount of compression such that the Droidish sentences are a little shorter in duration than the spoken English. But the word dictionary was built to be customizable so adding more words or specific vocabulary can increase the level of compression and therefore the speed at which information is communicated by the language.
The second system is like the first, but it uses a dictionary that maps characters to single tones, so that it can spell out any words that are not already defined in the first system. The encoder then adds distinct signals to the sentence indicating which system is being used. This allows the decoder to switch from system to system as it works through the sentence. A whistle is used to indicate the word-based encoding system should be used while a chirp is used to indicate the character system. For purely aesthetic reasons, I added a random number of whistles (up to 3) to be used as the signal.

I wrote a Medium article about the project [here](https://medium.com/@merwijas/the-utility-of-droidspeak-16ad4416d003)

# LD-1
Operating System for RBPi Droid (**L**LM **D**roid - 1)

This .py file should be run on the RBPi 5 powering the droid. I will be uploading a droid construction here guide soon. The LD-1 model requires a mic for Audio TTS input to receive verbal commands, a speaker for the LLM reponse to be played aloud in Droidish, and a screen for the text translation, first in Aerobesh and then in English. The LLM used here is OpenAI but a local LLM running on the pi works as well, just gives slow responses because it's running concurrently with vsok tts model. RPBi 5 16GB will likely not have that problem and I will use a local ollama model on that as soon as I can get one. A local LLM is better because it allows for offline use, memory systems, and retraining or reinforcement learning to improve the Droid LLM behavior. If you have questions please contact me or post in the repo discussion.
